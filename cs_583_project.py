# -*- coding: utf-8 -*-
"""CS_583_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ijwKbwTPzefQFrSwMEGffzc-_9ygMkNx
"""

import os
import numpy as np
from math import ceil
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

TRAIN_DIR = '/content/drive/MyDrive/Alzheimer_s Dataset/train'
TEST_DIR = '/content/drive/MyDrive/Alzheimer_s Dataset/test'

IMAGE_SIZE = 176
BATCH_SIZE = 10

datagen = ImageDataGenerator(rescale=1./255)

CLASSES = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']

def load_images_from_folder(folder, klass, num_images=4):
    images = []
    for filename in os.listdir(os.path.join(folder, klass))[:num_images]:
        img_path = os.path.join(folder, klass, filename)
        img = image.load_img(img_path, target_size=(IMAGE_SIZE, IMAGE_SIZE), color_mode='grayscale')
        img = image.img_to_array(img)
        img = np.expand_dims(img, axis=0)
        images.append(img)
    return np.vstack(images)

num_images_to_display = 8  # or any other number that you prefer

# Create a subplot grid
fig, axes = plt.subplots(nrows=len(CLASSES), ncols=num_images_to_display, figsize=(20, 10))

# Load and display the images
for i, klass in enumerate(CLASSES):
    imgs = load_images_from_folder(TRAIN_DIR, klass, num_images_to_display)
    for j in range(num_images_to_display):
        if j < imgs.shape[0]:  # Check if the image index is within the range of loaded images
            ax = axes[i, j]
            img = imgs[j] / 255.0
            ax.imshow(np.squeeze(img), cmap='gray')
            ax.axis('off')
        else:
            axes[i, j].axis('off')  # Hide axes if there are no more images to display

# Set class names as titles for the first column
for ax, klass in zip(axes[:, 0], CLASSES):
    ax.set_ylabel(klass, rotation=90, size='large')

plt.tight_layout()
plt.show()

def build_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())
    model.add(Dense(64))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model


model = build_model()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

train_generator = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    color_mode='grayscale',
    class_mode='categorical')

validation_generator = datagen.flow_from_directory(
    TEST_DIR,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    color_mode='grayscale',
    class_mode='categorical',
    shuffle=False
)

history = model.fit_generator(
    train_generator,
    steps_per_epoch=max(1, train_generator.samples // BATCH_SIZE),
    epochs=50,
    validation_data=validation_generator,
    validation_steps=max(1, validation_generator.samples // BATCH_SIZE))

model.save('alzheimers_model.h5')

evaluate = model.evaluate_generator(validation_generator, steps=validation_generator.samples // BATCH_SIZE, verbose=1)
print('Model Loss: {}, Model Accuracy: {}'.format(evaluate[0], evaluate[1]))

# Predict the test set
validation_generator.reset()
predictions = model.predict(validation_generator, steps=len(validation_generator))

# Convert predictions to class indices
predicted_classes = np.argmax(predictions, axis=1)

# Get the true class indices
true_classes = validation_generator.classes

# Get the confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Labels')
plt.xlabel('Predicted Labels')
plt.show()

"""
* The matrix is a 4x4 grid, which suggests there are four classes in the classification task. These are typically labeled from 0 to 3.

* The diagonal cells (top left to bottom right) represent the number of correct predictions for each class. In your case, 15 instances of class 0, 12 of class 1, 14 of class 2, and 15 of class 3 were correctly predicted.

* The off-diagonal cells show the number of incorrect predictions. In this matrix, there is only one incorrect prediction where an instance of class 2 was predicted as class 1.


* The columns represent the predicted class labels, and the rows represent the actual class labels. For example, the first column represents the predictions made by the model as class 0, and the first row represents the actual instances that are class 0.

* The color intensity represents the magnitude of the values, with darker colors typically representing higher numbers. Here, darker shades of blue correspond to a higher count of instances.

* This confusion matrix indicates that the model performed quite well, with most of the predictions being correct (as shown by the high numbers on the diagonal). There are few misclassifications, which suggests good model accuracy for this particular task.


"""

import os
import numpy as np
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, BatchNormalization
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Set the directories for the training and test images
TRAIN_DIR = '/content/drive/MyDrive/Alzheimer_s Dataset/train'
TEST_DIR = '/content/drive/MyDrive/Alzheimer_s Dataset/test'

# Set the image size and batch size
IMAGE_SIZE = 176
BATCH_SIZE = 10

# Advanced model architecture
def build_advanced_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization())

    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization())

    model.add(Conv2D(128, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization())

    model.add(Flatten())
    model.add(Dense(64))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4))  # Assuming 4 classes
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy',
                  optimizer=Adam(learning_rate=0.001),  # Experiment with learning rate
                  metrics=['accuracy'])
    return model

# Data augmentation for training data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Data generator for validation data (no augmentation)
test_datagen = ImageDataGenerator(rescale=1./255)

# Train generator
train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    color_mode='grayscale',
    class_mode='categorical'
)

# Validation generator
validation_generator = test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    color_mode='grayscale',
    class_mode='categorical',
    shuffle=False  # Important for correct label mapping
)

from sklearn.utils.class_weight import compute_class_weight

# Calculate the class weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)

class_weight_dict = {i : weight for i, weight in enumerate(class_weights)}

callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, verbose=1),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

# Train the model with class weights
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    epochs=50,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // BATCH_SIZE,
    class_weight=class_weight_dict,
    callbacks=callbacks
)

# After training, evaluate the model on the test set
evaluate = model.evaluate(
    validation_generator,
    steps=validation_generator.samples // BATCH_SIZE,
    verbose=1
)
print('Model Loss: {}, Model Accuracy: {}'.format(evaluate[0], evaluate[1]))

# Optionally, plot the training history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Training and Validation Loss')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

plt.show()

## Correctly calculate the number of steps to cover all samples in the validation set
validation_steps = ceil(validation_generator.samples / BATCH_SIZE)

# Reset the validation generator to ensure the order of the samples
validation_generator.reset()

# Predict the validation set results
predictions = model.predict(validation_generator, steps=validation_steps)

# Convert predictions to class indices
predicted_classes = np.argmax(predictions, axis=1)

# Ensure that true_classes and predicted_classes have the same length
true_classes = validation_generator.classes[:len(predicted_classes)]

# Generate and plot the confusion matrix
confusion_mtx = confusion_matrix(true_classes, predicted_classes)
plt.figure(figsize=(10, 8))
class_labels = list(validation_generator.class_indices.keys())
sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.ylabel('True Labels')
plt.xlabel('Predicted Labels')
plt.show()



"""## Resnet Model"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam

from google.colab import drive
drive.mount('/content/drive')

IMAGE_SIZE = 224
BATCH_SIZE = 32
NUM_CLASSES = 4  # Adjust based on your classes
EPOCHS = 20
TRAIN_DIR = '/content/drive/MyDrive/Alzheimer_s Dataset/train'
TEST_DIR = '/content/drive/MyDrive/Alzheimer_s Dataset/test'

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)
valid_datagen = ImageDataGenerator(rescale=1./255)

valid_generator = valid_datagen.flow_from_directory(TEST_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

valid_generator = valid_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)
x = Dropout(0.5)(x)
predictions = Dense(NUM_CLASSES, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor='val_loss', patience=10)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=valid_generator,
    callbacks=[early_stopping, model_checkpoint]
)

# Evaluate the model on the validation data
loss, accuracy = model.evaluate(valid_generator)

# Print the results
print(f"Validation Loss: {loss}")
print(f"Validation Accuracy: {accuracy}")

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Assuming predictions and true_classes are already defined
cm = confusion_matrix(true_classes, predicted_classes)

# Plotting the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()